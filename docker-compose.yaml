networks:
  data-quality-network:


services:
  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    hostname: zookeeper
    container_name: zookeeper
    networks:
      - data-quality-network
    # ports:
    #   - "22181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:6.2.0
    hostname: kafka
    container_name: kafka
    networks:
      - data-quality-network
    volumes:
      - ./kafka/scripts:/app/scripts # contains the script to create the kafka topics
    # ports:
    #   - "29092:29092"
    #   - "9092:9092"
    #   - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      TOPIC_TIMESTAMP_TYPE: 'CreateTime'
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
    depends_on:
      - zookeeper
    env_file:
      - .env

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8080:8080
    environment:
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      - data-quality-network

  stream:
    build:
      context: .
      dockerfile: dockerfiles/stream.Dockerfile
    container_name: stream
    depends_on:
      - kafka
    networks:
      - data-quality-network
    environment:
      - INPUT_TOPIC=${INPUT_TOPIC}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - WINDOW_DURATION=${WINDOW_DURATION}
    env_file:
      - .env
    volumes:
      - ./stream/datasets:/app/datasets:ro
      - ./execution-results/stream-daq/:/app/blast-logs/
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: "4.0"

  daq:
    build:
      context: .
      dockerfile: dockerfiles/daq.Dockerfile
    container_name: daq
    depends_on:
      - kafka
      - postgres-dq
      - output-consumer
    networks:
      - data-quality-network
    environment:
      - INPUT_TOPIC=${INPUT_TOPIC}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - SPARK_NUM_CORES=${SPARK_NUM_CORES}
      - MEMORY_LIMIT=${MEMORY_LIMIT}
    env_file:
      - .env
    volumes:
      - ./execution-results/stream-daq:/data
    deploy:
      resources:
        limits:
          memory: ${MEMORY_LIMIT}
          cpus: ${SPARK_NUM_CORES}

  postgres-dq:
    image: postgres:15
    container_name: postgres-dq
    networks:
      - data-quality-network
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: dq_user
      POSTGRES_PASSWORD: dq_pass
      POSTGRES_DB: dq_db
    volumes:
      - ./postgres-data:/var/lib/postgresql/data  # persist database files
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro  # optional init script
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U dq_user -d dq_db" ]
      interval: 5s
      timeout: 5s
      retries: 5

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    networks:
      - data-quality-network
    ports:
      - "5050:80"  # access it at http://localhost:5050
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    depends_on:
      - postgres-dq

  output-consumer:
    build:
      context: .
      dockerfile: dockerfiles/output-consumer.Dockerfile
    container_name: output-consumer
    depends_on:
      - kafka
    networks:
      - data-quality-network
    environment:
      - OUTPUT_TOPIC=${OUTPUT_TOPIC}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - POSTGRES_HOST=postgres-dq
      - POSTGRES_DB=dq_db
      - POSTGRES_USER=dq_user
      - POSTGRES_PASSWORD=dq_pass
    volumes:
      - ./execution-results:/app/data
    env_file:
      - .env